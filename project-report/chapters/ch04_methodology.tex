%!TEX root = ../report.tex
\begin{document}
\chapter{Methodology and Experimental Setup}
\label{chapter:methodology}
This chapter will discuss the RandlLA-Net used for 3D semantic segmentation, especially its network architecture and how it helps in effective segmentation. 
We also discuss the details of Deep Ensembles and Flipout methods for uncertainty quantification.
We study the evaluation metrics for 3D semantic segmentation and OOD detection and the methods to generate OOD scores, such as Maximum Softmax Probability (MSP) as a baseline method and Entropy.
Finally, we conclude this chapter by discussing the experiment setup details of RandLA-Net with Deep Ensembles and Flipout.
\section{RandLA-Net}
\label{sec:meth_randla}
As stated in \cite{Hu_2020_CVPR_Randla}, it is a lightweight and efficient neural network architecture for semantic segmentation of 3D point clouds.
From Section~\ref{sec:dl_approach}, we can observe that the RandLA-Net architecture is best performing among the point models.
Efficient computation, memory usage and a model with direct application to 3D points are the primary motivation when developing the RandLA-Net.
RandLA-Net employs random point sampling and the local feature aggregation module to achieve these goals.
Authors in \cite{Hu_2020_CVPR_Randla} proved that a successive application of random point sampling along with the local feature aggregation module effectively reduces and extracts the features of the large scale point clouds from a scale of $10^5$ to $10^2$.
Random point sampling is only applied during the preprocessing of the dataset by subsampling the point cloud by a factor of 0.6.

In random point sampling, We select K points uniformly from the original point cloud in random point sampling and have a computational complexity time of O(1).
Random point sampling has the lowest computational complexity and computation time independent of the number of points compared with other point sampling methods.
Random point sampling has a significant disadvantage of dropping essential points despite these advantages.
To overcome this, the authors of RandLA-Net proposed a local feature aggregation module for the progressive capture of complex features on these selected points.
\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
            \includegraphics[scale=0.4, angle=90]{images/localfeatueaggregation-randlanet.png}
            \caption{}
            \label{fig:randlanetlfa}       
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
            \includegraphics[scale=0.55, angle=90]{images/randlanet.png}
            \includegraphics[scale=0.55, angle=90]{images/archi_expl.png}
            \caption{}
            \label{fig:networkarchitecture}
    \end{subfigure}
    \caption{Illustration of (a) local feature aggregation module in RandLa-Net and (b) architecture of RandLA-Net. Both the images are taken from \cite{Hu_2020_CVPR_Randla}.}
\end{figure}

Figure \ref{fig:randlanetlfa} represents the local features aggregation module for the RandLA-Net.
This module is applied parallelly on the 3D points, and the architecture of the local feature aggregation module is further divided into three sub-modules.
They are local spatial encoding (LocSE), attentive pooling and dilated residual blocks represented as green, pink and blue blocks, respectively, in Figure \ref{fig:randlanetlfa}.
Let us discuss further each of these submodules in detail.

\subsection{Local Spatial Encoding}
The local spatial encoding (LocSE) module takes each point ($p_i$) in the point cloud (P) and encodes its neighbouring point's position (x, y and z).
This encoding makes sure that point p always has information about its neighbours.
Also, this encoding helps in learning geometric patterns and learning complex structures progressively.
This module works in three steps:
\begin{enumerate}
    \item Finding nearest neighbours
    \item Relative position encoding
    \item Feature augmentation
\end{enumerate}

In step 1, neighbour points for point ($p_i$) are collected using the Euclidean distance-based K-Nearest Neighbour (KNN) algorithm.
Step 2 encodes these collected K-points for point ($p_i$) using a Multi-Layer Perceptron (MLP) into relative point position. The encoding formula is given by
$$
r_i^k = MLP(p_i \oplus p_i^k \oplus (p_i - p_i^k) \oplus ||p_i-p_i^k||)
$$
where $r_i^k$ is the relative position of point $p_i$ with respect to $p_i^k$, here in $p_i$ and $p_i^k$ only the x,y and z positions are used.
$\oplus$ indicate the concatenation operation, and $||p_i-p_i^k||$ represents Euclidean distance between $p_i$ and $p_i^k$ respectively.
This step 2 of relative position encoding is represented by the above part in the LocSE module in the green track in Figure \ref{fig:randlanetlfa}.
Step 3 creates an augmented feature vector $\hat{f_i^k}$ by concatenation of relative point position ($r_i^k$) and its point features ($f_i^k$) of point $p_i^k$.
Point features ($f_i^k$) include the R, G and B values and other features such as intensity values.
This step 3 is represented in the lower part of the LocSE module in the yellow track in Figure \ref{fig:randlanetlfa}.
\subsection{Attentive Pooling}
This augmented feature vector $\hat{f_i^k}$ from the LocSE module is passed through a pooling layer to extract essential features.
Authors state that the use of max-pooling and mean-pooling layers leads to loss of information. Because of this, the authors made use of the attention mechanism, which helps in learning important features automatically.
Given the feature vector $\hat{f_i^k}$, a function $g$ is learned with the help of MLP and softmax, and the resultant vector is denoted as $s_i^k$ in the pink block in Figure \ref{fig:randlanetlfa}.
Scores $s_i^k$ calculated from function $g$, is multiplied with feature vector $\hat{f_i^k}$ and the resultant is called informative feature vector and summed up to form a unique feature vector $\tilde{f_i}$ for point $p_i$ and this operation is mathematically denoted as
$$
s_i^k = g(f_i^k, W)
$$
$$
\tilde{f_i}= \sum_{k=1}^K (\hat{f_i^k}.s_i^k)
$$
Where $g$ is the softmax function and $W$ is the attention weights, $\hat{f_i^k}$ is the input feature vector to the attentive pooling module, $s_i^k$ is the attention scores and finally $\tilde{f_i}$ is the output feature vector from the attentive pooling module.

\subsection{Dilated Residual Block}
Dilated Residual Block is a ResNet inspired module claimed by authors and represented as a blue colour module in Figure \ref{fig:randlanetlfa}.
This module combines multiple LocSE, Attentive Polling, and a skip connection that feeds an informative feature vector to output.
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/dilatedresidualblock.png}
    \caption{Image depicting the working of Dilated residual block with each circle representing the receptive field of the block for feature extraction. LA represents the combination of Local Spatial Encoding and Attentive Pooling modules combined. Image taken from \cite{Hu_2020_CVPR_Randla}.}
    \label{fig:dilatedresidualblock}
\end{figure}
Let us consider a red point in Figure \ref{fig:dilatedresidualblock}. After applying the first LocSE and Attentive Pooling module, it observes that K neighbours are represented in the red circle.
A secondary application of LocSE and Attentive Polling allows the red point to observe $K^{2}$ neighbours represented as a large red circle in the right subimage in Figure \ref{fig:dilatedresidualblock}.
This progressive dilation of receptive fields allows for observing local features in the first application of LocSe and Attentive Pooling and then observing global features on further application of LocSE and Attentive Polling modules.
Authors claim that the more LocSE and Attentive Pooling stacked in Dilated Residual Block powerful the Dilated Residual Block becomes and greater the receptive field at the expense of computational time.
Authors also claim that only the stacked application of two LocSE and Attentive Pooling modules is powerful enough, and it is effective and efficient in computational time.

To summarize, up to this point, we have studied the unique feature of RandLA-Net. That is how random point sampling in conjecture with local features aggregation module in Figure \ref{fig:randlanetlfa} helps in the extraction of features progressively.
We also studied how the local feature aggregation module is sub-divided into LocSE, Attentive Pooling and Dilated Residual Block. Each of these submodule working procedures.
In the next section, we study the architecture of RandLA-Net.

\subsection{RandLA-Net Architecture}
RandLA-Net is an encoder-decoder architecture with skip connections used in various segmentation networks such as 3D U-Net\cite{wang2018two_3DUnet}.
The input point clouds are directly applied to the encoder consisting of Fully Connected (FC) and four Local Feature Aggregation (LFA) modules are connected sequentially.
The point cloud size reduces by four for every encoder layer. 
Similarly, four decoder layers are used, and the input features maps to each decoder layer are upsampled and concatenated to respective encoder feature maps via skip connections.
The MLP is applied and fed into the next decoder layer.
The output of the final decoder layer is fed into three FC layers for point classification, and a Dropout layer is added before the last layer with a Dropout rate of 0.5.
The detailed network architecture is illustrated in Figure \ref{fig:networkarchitecture}.

We chose RandLA-Net because of the following reasons:
\begin{enumerate}
    \item Efficient extraction of complex structures progressively using Local Feature Aggregation (LFA) module.
    \item Fewer parameters (1.24M) compared to other SOTA 3D semantic segmentation models, making training efficient, as 3D semantic segmentation models are computationally expensive.
    \item Proven performance over various datasets such as Semantic3D and SemanticKITTI, along with ablation study of each submodule in LFA proposed in \cite{Hu_2020_CVPR_Randla}.
    \item No preprocessing such as range image representation as in \cite{Milioto2019} or farthest point sampling with a computational complexity of $O(N^2)$ as in \cite{Qi_2017_CVPR_pointnet}. RandLA-Net employs random point sampling with a computational time of $O(1)$.
    \item State of the art performance in point-based methods, consisting of Multi-Layer Perceptrons (MLP) and without expensive operations such as kernelization or graph construction.
\end{enumerate}

\subsection{3D Semantic Segmentation Evaluation Metrics}
To evaluate the performance of RandLA-Net over the training dataset (Semantic3D in our case), we chose two metrics.
They are Mean Intersection-over-Union (mIoU) and Accuracy.

\subsubsection{Mean Intersection-over-Union (mIoU)}
Mean Intersection-over-Union is a widely used metric for performance evaluation in the task of semantic segmentation.
It is calculated as a mean of a fraction of the intersection area between predicted and ground-truth masks and the union of predicted and ground truth masks.
mIoU is calculated as
$$mIoU=\frac{1}{N}\sum_{k=1}^N \frac{p_k\cap g_k}{p_k \cup g_k}$$
Where $N$ is the number of classes, $p_k$ and $g_k$ are predicted mask and ground truth mask of $k^{th}$ class.

\subsubsection{Accuracy}
Accuracy is another widely used metric, which can be quantified as a number of points in the point cloud correctly classified.
It can be formulated from the confusion matrix as
$$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$
Where TP, TN, FP and FN are True Positives, True Negatives, False Positives and False Negatives, respectively, from the confusion matrix.
This metric alone can be misleading in case of severe class imbalance, so mIoU is used in conjecture.

Here, we conclude the study of RandLA-Net, the reason for its effective performance, argue the reasons to chose RandLA-Net and briefly discuss the evaluation metrics used.
In the following sections, we will discuss the utilized uncertainty estimation methods such as Deep Ensembles, Flipout and metrics used to evaluate 3D semantic segmentation and OOD detection.
\section{Deep Ensembles}
\label{sec:meth_deepensembles}
Deep Ensembles employ a kind of ensemble learning technique and are proposed in \cite{lakshminarayanan2016simple}.
Like bagging, the idea is to train the same network with the same data with random initializations N number of times.
These N trained models converge similarly with little difference given the same training conditions and hyperparameters.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/DE.jpg}
    \caption{Illustration of test dataflow in Deep Ensembles, where input point cloud is fed into multiple randomly initialized models ($M_1-M_n$). 
    These individual predictions from each model are averaged to compute the final prediction.}
    \label{fig:deepensembles}
\end{figure}
An example of a deep ensemble is depicted in Figure \ref{fig:deepensembles}.
Here the input point cloud is fed into N number of models. In our case, these models are all RandLA-Net.
The resulting predictions are combined to get the final prediction. 
The combination is done by averaging over all the predictions of N models to get the final predictions in our case.
Deep Ensembles are proven to improve the overall performance of the model as in \cite{bhandary2020evaluating}, and we also expect the same behaviour in our case.

Despite their performance boosting ability, they are also used to estimate uncertainty as in \cite{lakshminarayanan2016simple}.
\cite{lakshminarayanan2016simple} proposes that with the increase in the number of ensembles, the Negative Log-Likelihood (NLL) and Brier Score (BS) goes down, suggesting the network produces well-calibrated predictions.
Apart from use of entropy for measuring uncertainty, NLL and BS are two other popular uncertainty quantification methods.
For a perfectly trained and calibrated network we expect all the entropy, NLL and BS to be as low as possible.
\cite{lakshminarayanan2016simple} also studies the effect of entropy with out-of-distribution (OOD) classes.
They performed the study on MNIST-NotMNIST and SVHN-CIFAR10, with the first dataset in the pair being ID and the second dataset being the OOD dataset.
The authors verified that entropy distribution on the ID dataset is peaky. Similarly, the entropy distribution on the OOD dataset is more spread across all entropy values.
We expect that similar performance to be observed in 3D semantic segmentation also.
Because their proven ability to classify OOD on the classification task, boosts the model's performance and their ease of implementation makes Deep Ensembles an ideal candidate for OOD detection in 3D semantic segmentation.

\section{Flipout}
\label{sec:meth_flipout}
In this thesis, we also employed the Flipout version of the RandLA-Net model for uncertainty estimates, and in this section, we discuss Flipout and its application.
Flipout has initially been introduced in \cite{Flipout} as an efficient method of gradient decorrelation in a mini-batch of examples.
This effect is implemented by adding independent weight perturbations sampled from distribution for each example.
Authors in \cite{Flipout} describe weight perturbations as methods whose weights of the neural network are sampled from distribution during training.
In general, neural network weights are point estimates. In the case of weight perturbations, they are modelled as a distribution where each weight $W=\overline{W}+\Delta W$ in which $\overline{W}$ is the mean weight and $\Delta W$ is the perturbation.
If this $\Delta W$ is sampled from a Gaussian distribution, it is called a Gaussian perturbation.
Since the sampled weight perturbation is the same for all the examples over the mini-batch, the authors argue a correlation between the gradients.
So authors of Flipout make two critical assumptions to deal with this problem:
They are, perturbation of different weights should be different, and the distribution of perturbation should be symmetric around zero.
So under these assumptions, the authors observed that the perturbation distribution is invariant to element-wise multiplication of the random sign matrix.
Here the random sign matrix is defined as a matrix whose elements are randomly sampled uniformly from $\pm 1$. 
So the weight perturbation is modelled as Equation~\ref{eq:flipout_main} in \cite{Flipout}, where $\widehat{\Delta W}$ is the base perturbation which is the same for all weights, $\circ$ represents element-wise multiplication, $r_n, s_n$ represent the randomly signed vectors of ($\pm 1$) and $\Delta W_n$ is the perturbation of $n^{th}$ weight.
\begin{equation}
    \Delta W_n = \widehat{\Delta W} \circ r_n s_{n}^T
    \label{eq:flipout_main}
\end{equation}
So the node output of the neural network is modified as 
$$y_n = \phi(W^T x_n) $$
$$y_n = \phi((\overline{W}+\widehat{\Delta W} \circ r_n s_{n}^T)^Tx_n)$$
%$$y_n = \phi(\overline{W} x_n+(\widehat{\Delta W}^T (x_n\times s_{n}))\times r_n)$$
Here $\phi$ is the activation function, and $x_n$ is the $n^{th}$ input example in the mini batch.
Also the vectors $r_n$ and $s_n$ are independent of $\overline{W}$ and $\widehat{\Delta W}$ which does not add any additional terms in backpropagation.
%This whole operation is vectorized and is stated as a matrix multiplication in Equation~\ref{eq:fowpass_flipout} where $R$ and $S$ consist of rows of $r_n$ and $s_n$.
%Here $R$ and $S$ are independent of $\overline{W}$ and $\widehat{\Delta W}$ which does not add any additional terms in backpropagation.
% \begin{equation}
%     Y = \phi(X\overline{W}+((X\times S)\widehat{\Delta W})\times R)
%     \label{eq:fowpass_flipout}
% \end{equation}
Authors also argue that the Flipout reduces the variance in gradients when compared to shared perturbation but has a slightly higher variance than independent perturbations along with the mathematical proof produced in \cite{Flipout}.


At a glance, these random weight perturbations give us slightly different output estimates for every forward pass.
In our case, we changed the few classification layers of the network to Flipout compatible more about the setup in Section \ref{sec:flipout_setup}.
We then compute the forward pass multiple times, and these multiple output estimates from multiple forward passes are then averaged to get the final prediction estimate.
A representation of this operation is presented in Figure~\ref{fig:flipout_ex}, and here, $F_1$ represents the Flipout trained version of RandLA-Net being used for N forward passes.
These classifications are averaged to get the final classification score.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/flipout.jpg}
    \caption{Illustration of test dataflow in Flipout. Here $F_1$ represents the Flipout trained model and we compute $n$ forward passes of the same point cloud on $F_1$.
    These individual predictions are averaged to compute the final predictions.}
    \label{fig:flipout_ex}
\end{figure}
\FloatBarrier

\section{OOD Estimates}
In this section, we discuss the two methods used to generate the OOD score for classifying the OOD objects and how these scores are calculated.
The two metrics are Maximum Softmax Probability (MSP) and Entropy.

\subsection{Maximum Softmax Probability}
First proposed in \cite{hendrycks2016baseline_MSP}, Maximum Softmax Probability (MSP) uses the probability of the classification from the Softmax for OOD detection. 
MSP still serves as the most evaluated baseline method for OOD detection, so we want to incorporate this score. 
Here the main idea is that if a point in a point cloud is from ID data, then it will have a higher MSP score and vice versa.
It is calculated as a given in Equation~\ref{eq:MSP_formula}, where $y_n$ is the list of softmax probability scores for each class given as $p_1, p_2, .., p_n$.
$$y_n = [p_1, p_2,..,p_n]$$
\begin{equation}
    MSP = max(y_n)
    \label{eq:MSP_formula}
\end{equation}

\subsection{Entropy}
\label{sec:meth_entropy}
Entropy is defined by \cite{entropy_robinson} as an ``ill-defined notion of chaos or uncertainty''.
Entropy has its roots in thermodynamics, and in computer science, it can be used as a measure of information content.
In deep learning, entropy is used as a measure for uncertainty and calculated as given in Equation~\ref{eq:ent_calc}, where $P(x_i)$ is softmax probability for $i^{th}$ of point x.
\begin{equation}
    Entropy = -\sum_i P(x_i) log P(x_i)
    \label{eq:ent_calc}
\end{equation}
In theory, following this formula, if the point in the point cloud is from In-Distribution (ID), the softmax output is higher for one class, and other classes will be near zero.
So this point from ID has lower entropy score because the softmax probabilities are nearly zero for all the classes expect for one.
Similarly for the OOD point, the softmax probabilities are spread across all the classes because the network doesn't know which class it belong to. So the entropy for OOD point is higher.

\section{OOD Detection Evaluation Metrics}
This section will discuss the metrics used for evaluating OOD detection. Firstly the Receiver Operating Characteristic (ROC) curve to extract thresholds of the score and later the Area Under the ROC curve (AUROC) for evaluation and comparison of the OOD detection performance.
\subsection{Receiver Operating Characteristic Curve}
ROC curves are typically used for selecting the best classifiers based on their performance.
ROC curves were historically used in signal detection but later found their way into the machine learning community.
According to \cite{ROC_example}, this is because accuracy itself is a poor metric to realise the model's performance.
Also, ROC curves are insusceptible to change in class distribution, that is, the proportion of negative to positive examples.
ROC curves are drawn based on True Positive Rate (TPR) and False Positive Rate (FPR).
TPR is calculated as the number of positively classified samples to the total positive samples. FPR is calculated as the number of negatively classified samples to the total number of negative samples.

A ROC curve is generated, given a set of probabilities/scores and their true labels which are either positive or negative.
We then generate a range of threshold values from these probabilities and compute TPR and FPR for each threshold value.
These TPR and FPR are plotted to generate a ROC curve as displayed in Figure~\ref{fig:ROC_curve_example}. 
Figure~\ref{fig:ROC_curve_example} represents ROC curves for various classifiers, and here the red line represents the ROC curve for the random classifier. The green line represents the roc curve for a perfect classifier.
Since we want our classifier to perform better than a random classifier, we expect our ROC curve to be in the blue region.
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.35]{images/ROC_curve_example.pdf}
    \caption{Illustration of example ROC curves with red curve for random classifier and green curve representing the perfect classifier.
    Our classifier ROC curve is expected to be placed in this blue shaded region.}
    \label{fig:ROC_curve_example}
\end{figure}

\subsection{Area Under an ROC curve}
Since the ROC curves are a two dimensional depictions and we need a single scalar value to compare the performance of the classifiers, we use the Area Under a ROC curve (AUROC) value.
AUROC is the area enclosed below the ROC curve, and the AUROC value ranges between 0 and 1 as the curve is plotted in a unit square.
AUROC for a random classifier's ROC curve as depicted in red colour in Figure~\ref{fig:ROC_curve_example} is 0.5, and for the perfect classifier depicted as the green line is one, because its area encompasses the whole square.
In this thesis, we use this score to evaluate and compare the performance of OOD detection between Deep Ensembles and Flipout.

\section{Experimental Setup}
    %\subsection{Training parameters}
    In the previous sections, we discussed how RandLA-Net works and also the details of Deep Ensembles and Flipout.
    This section will discuss the libraries used and training parameters of the RandLA-Net for Deep Ensembles, Flipout and Dropout.    The training and testing of RandLA-Net is time and resource-intensive.
    So, we used the High-Performance Computing (HPC) GPU cluster from DFKI, especially the Nvidia Titan-XP and the Nvidia-Tesla V100.
    \subsection{Libraries}
    With the purpose of reproducibility in mind, here we provide the list of major libraries used and their versions used for training.
    We also added the requirements.txt in the attached code for ease of reproduction as it is a struggle to set up the environment.
    \begin{enumerate}
        \item Python - 3.6
        \item Tensorflow - 1.15.0
        \item Tensorflow probability - 0.7.0
        \item Open3d-python - 0.3.0 (training), 0.13.0 (visualizations)
    \end{enumerate}
    
    \subsection{Deep Ensembles}
    \label{sec:de_setup}
    For Deep Ensembles, we trained 20 randomly initialized instances of RandLA-Net on the Semantic3D dataset.
    We used the default training parameters as provided by the authors, and they are a learning rate of 0.01 with the decay of 0.95 multiplied for every epoch, batch size of 4 and trained for 100 epochs.
    We changed the pipeline to infer the whole point cloud and save the probability scores from these 20 models.
    This change is made because the setup provided by authors infer only few points and then smoothen these softmax scores to the neighbouring points of inferred points.
    These saved probabilities from 20 models are averaged, and Maximum Softmax Probability (MSP) and Entropy are calculated.
    Out of these 20 models, we provide the evaluation for every $5^{th}$ model, i.e., 1, 5, 10, 15 and 20.
    \subsection{Flipout}
    \label{sec:flipout_setup}
    For Flipout-versioned RandLA-Net, we replaced the last three classification layers of the RandLA-Net highlighted with the red box in Figure~\ref{fig:fout_randlanet} with their Flipout counterparts from Tensorflow Probability.
    In addition to this change, we also removed the Dropout layer after the second classification layer.
    We chose these three layers because they are significant decision-makers for classification, and the remaining network is a feature extraction module.
    Following the same training procedure stated by the authors, we observed similar performance, so we decided not to change any training parameters.
    However, we initialized the Flipout weight perturbations with a normal prior of standard deviation 1.
    We also tried this with various standard deviations such as 0.1, 0.5, 1, 1.5, and 2.0 and found that standard deviation values of 0.5 and 1.0 result in better convergence.
    In comparison to Deep Ensembles, we performed 20 forward passes for the Flipout versioned RandLA-Net model, then averaged the results and represented every $5^{th}$ forward pass.
    \begin{figure}
        \centering
        \includegraphics[scale=0.42]{images/fout_randlanet.png}
        \caption{Flipout-versioned RandLA-Net where the last three FC layers as depicted in red box are made Flipout-compatible. Image taken from \cite{Hu_2020_CVPR_Randla}.}
        \label{fig:fout_randlanet}
    \end{figure}
    %The Dropout version of the RandLA-Net setup is described in Section~\ref{sec:randladout} as it is only used for the evaluation of OOD detection more as a baseline method.
    \subsection{Dropout}
    \label{sec:randladout}
    The original architecture of the RandLA-Net has only one Dropout layer at the end of the second classification layer, represented as DP in Figure~\ref{fig:networkarchitecture}.
    We also used the same setup for our Dropout experiment with a Dropout probability of 0.5.
    Similar to Flipout, we performed 20 forward passes on the dropout, then averaged and represented every AUROC performance on every $5^{th}$ forward pass.

\end{document}
