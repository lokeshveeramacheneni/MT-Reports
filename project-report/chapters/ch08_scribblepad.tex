%!TEX root = ../report.tex

\begin{document}
\chapter{Notes/Remarks}

\section{Introduction}
The development of Deep Neural Networks (DNNs) made tasks such as object classification and object detection simple.
These DNNs has seen their way to various real world scenarios such as autonomous driving \cite{autonomousdriving}, semi-autonomous robotic surgery \cite{roboticsurgery} and also in space rovers \cite{Marsrover_1}, \cite{Marsrover_2}.
DNNs are majorly deployed in the perception stack in the autonomous pipeline. 
Figure \ref{fig:Apollopipeline} depicts the pipeline of the modules present in one of the open source autonomous driving platform called Apollo \cite{baiduapollo}.
From this pipeline, we can infer that the most of the decisions regarding the vehicle control made by autonomous system is dependent on the output of the perception module.
Since the perception module plays such significance, the developers of the perception stack must make sure that the output is flawless.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.35]{images/Apollopipeline.png}
    \caption{Module pipeline for Apollo autonomous driving platform. Image taken from \cite{baiduapollo}}
    \label{fig:Apollopipeline}
\end{figure}

The DNNs deployed in perception module are needed to be trained on the dataset which should be similar to area of its deployment.
For example, an autonomous driving agent must be trained on dataset containing roads, vehicles, vegetation and other objects found around road.
This closedness of the dataset i.e., fixed number of classes, will cause an issue when the DNN encounter an unknown object in real world.
This unknown object is predicted as one of the class in the dataset, leading to radical decisions when this error is propogated down the pipeline in Figure \ref{fig:Apollopipeline}.
One such real world problem is encountered by the Tesla autonomous driving platform.
\begin{figure}[h!]
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[scale=0.5]{images/tesla_1.png}
        \caption{}
        \label{fig:teslafails_1}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[scale=0.5]{images/tesla_2.png}
        \caption{}
        \label{fig:teslafails_2}
    \end{subfigure}
    \caption{Tesla fails. Images taken from \cite{tesla_fails}}
\end{figure}

Figures \ref{fig:teslafails_1} and \ref{fig:teslafails_2} depict the misdetections from the Tesla autonomous driving system.
The problem with first one, is the moon is detected as the yellow signal light and second one has the problem of misdetection of burger king sign as stop signal.
These misdetections of unknown objects might lead to consequences beyond imagination.
This questions the safety of the Deep Neural Networks (DNNs) predictions.
An effort has been made in this thesis to detect these unknown objects in 3D LiDAR data using uncertainty score.
The unknown obejcts in the real world which are not present in the training dataset are called as out-of-distribution (OOD) class. 
More discussion on the OOD is presented in Section \ref{sec:oodvanom}.
More discussion on misdetections in a DNN trianed on MNIST and tested on USPS is presented in Section \ref{sec:mnistvusps}
The contributions made in this thesis are
\begin{enumerate}
    \item A survey on the available 3D LiDAR datasets and benchmark dataset for the OOD detection.
    \item A survey on the 3D semantic segmentation models, uncertainty estimation methods and classical OOD methods.
    \item Use of uncertainty for OOD detection in RandLA-Net
\end{enumerate}
\section{Introduction-OOD/Anomaly/Distributional shift}
\label{sec:oodvanom}
\begin{figure}[h!]
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/old_ship.jpg}
        \caption{}
        \label{fig:old_ship}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/Trainer_cruiser.jpeg}
        \caption{}
        \label{fig:trian_cruiser}
    \end{subfigure}
%\end{figure}
%\begin{figure}[h!]
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/Anomaly_container.jpg}
        \caption{}
        \label{fig:anom_container}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/Anomaly_titanic.jpg}
        \caption{}
        \label{fig:anom_titanic}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/ood_submarine.jpg}
        \caption{}
        \label{fig:ood_submarine}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/ood_airship.jpg}
        \caption{}
        \label{fig:ood_airship}
    \end{subfigure}
    \caption{Illustration of distributional shift, anomaly and out of distribution examples using various kind of ships. \ref{fig:old_ship} represents the sail ship during 18th century. \ref{fig:trian_cruiser} depicts the current training data.
    \ref{fig:anom_container}, \ref{fig:anom_titanic} represents the anamolous ship data and \ref{fig:ood_submarine}, \ref{fig:ood_airship} represents the OOD data. Images are taken from \cite{old_ship}, \cite{train_cruiser}, \cite{container},
    \cite{titanic}, \cite{submarine}, and \cite{airship} respectively in the order they appear.}
\end{figure}

Let us time travel back to $18^{th}$ century and assume that we had implemented a model to detect ships, the dataset images for the trained model will be similar to Figure \ref{fig:old_ship}.
$18^{th}$ century ships as in \ref{fig:old_ship} can be defined as ``\textit{ship contains hull and sails}''.
Fast forward to present time, current ships are as shown in Figure \ref{fig:trian_cruiser}.
Ship as in \ref{fig:trian_cruiser} can be defined as ``\textit{ship contains hull and passenger decks stacked upon each other}''.
Now if we want to deploy the old model trained with old ships to detect the present generation of ships, it is diffcult because of the change in definition and properties of ship. 
This change in data distribution over a period of time is called ``\textit{distributional shift}'' of the data.

Anomaly can be defined as the patterns that doesn't conform to the expected training behavior.
By this definition, Figure \ref{fig:anom_container} and Figure \ref{fig:anom_titanic} can be considerd as anomalies.
This is becuase Figure \ref{fig:anom_container} is a container ship looking similar to Figure \ref{fig:trian_cruiser} instead of passenger decks we have containers stacked.
Figure \ref{fig:anom_titanic} is also anomaly because the Titanic also has a hull, passenger decks and chimneys. 
This additional chimnies as a fetures deviates this image from the definition of the ship and can be considered as ``\textit{anomaly}''.

The input for out of distribution (OOD) is drawn from an unknown distribution of unknown data, which is not near to the trianing distribution.
Figures \ref{fig:ood_submarine} and \ref{fig:ood_airship} are submarine and ariship which are from unknown distribution and they doesn't adhere to the definition of ship by any means.
In general, one can argue that OOD can be defined as inputs which doesn't belong to any class in the training data.

%\begin{figure}[h!]
%    \begin{subfigure}{0.333\textwidth}
%        \centering
%        \includegraphics[height=0.3\textheight,width=0.98\textwidth]{images/intro_ood_anomaly/normal_train.png}
%        \caption{}
%        \label{fig:normal_train_time}
%    \end{subfigure}
%    \begin{subfigure}{0.333\textwidth}
%        \centering
%        \includegraphics[height=0.3\textheight,width=0.98\textwidth]{images/intro_ood_anomaly/anomaly_train.png}
%        \caption{}
%        \label{fig:anomaly_time}
%    \end{subfigure}
%    \begin{subfigure}{0.333\textwidth}
%        \centering
%        \includegraphics[height=0.3\textheight,width=0.98\textwidth]{images/intro_ood_anomaly/ood_train.png}
%        \caption{}
%        \label{fig:ood_time}
%    \end{subfigure}
%    \caption{Illustration of anomaly and OOD with time series data as example. \ref{fig:normal_train_time} depicts the triaining data as sinusoidal wave.
%    \ref{fig:anomaly_time} represents the anomaly in the sinusoidal wave and \ref{fig:ood_time} represents the square wave as OOD signal}
%\end{figure}

\section{Problem Statement}
In this thesis, we study the application of out-of-distribution (OOD) detection over the 3D semantic segmentation problem in the context of autonomous driving.
Notably, we study the 3D semantic segmentation datasets available and create a benchmark for in-distribution and out-distribution for the OOD setting.

The other major issue, we address in this thesis is the OOD detection methods themselves.
Existing OOD detection methods are developed on 2D classification tasks and applicability of these methods on 3D semantic segmentation tasks is not studied. 
This is also challenging because the existing OOD methods are not easily adaptable to the 3D segmentation models because segmentation involves multi class classification and moreover high dimensionality of the 3D data.
\newline

The research questions answered by this thesis are:
\begin{itemize}
    \item[\textbf{R1}] How to create a benchmark over 3D segmentation datasets for the OOD setting?, i.e., create the in-distribution and out-distribution datasets.
    \item[\textbf{R2}] How to extend current OOD detection methods from 2D classification task to 3D semantic segmentation?
    \item[\textbf{R3}] Is uncertainty quantification an effective approach to classify OOD detection in 3D semantic segmentation models?
    % \item[\textbf{R4}] What metrics can be applied for the OOD detection task over 3D semantic segmentation models? 
    \item[\textbf{R4}] How to evaluate the OOD detections over the 3D semantic segmentation task?
\end{itemize}
\section{Related work - Datasets}
%Since the advent of AlexNet \cite{alexnet}, deeplearning has been in the rise. 
%Deep convolutional neural networks are the desired option for image analysis tasks such as classification, detection and segmentation.
%These deep convolutional networks are successful especially because of two reasons. 
%First is the architectures and their paralellisation, allowing them to train millions of images on a GPU.
%Second reason is the development of huge public benchmark datasets such as Imagenet \cite{imagenet}, Pascal VOC \cite{pascalvoc} and COCO \cite{coco} datasets
%Although deep convolutional networks are huge success in image analysis tasks, they perform poorly on 3D point cloud data.
LiDAR is one of the central component in the sensor suite for SLAM system in robotic applications \cite{thrun2006stanley}, \cite{patz2008practical}, \cite{hess20162dSLAM} and autonomous driving \cite{li2016vehicle}.
3D LiDAR data is preferred because, it can provide the exact replica of 3D geometry of the real world represented in the form of 3D point clouds.
Because of these rich features and widespread use of LiDAR sensors, tasks such as 3D object detection \cite{zhou2018voxelnet}, \cite{PIXOR} and 3D semantic segmentation \cite{qi2017pointnet++}, \cite{3Dmininet} are becoming more predominant area for research.

In this section, we will discuss about the available 3D LiDAR datasets for 3D semantic segmentation task and classify the datasets based on acquisition methods as in \cite{survey3d}.
\cite{survey3d} classifies the available public datasets into three classes based on the data acquisition process.
They are \textit{Sequential}, \textit{Static} and \textit{Synthetic} datasets.
The data for sequential datasets are collected as frame sequences where mechanical LiDAR is mounted on top of a autonomous driivng platform as in Figure \ref{fig:seq_data_lyft}.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.25]{images/sequential_lyft.png}
    \caption{Sequential mounted LiDAR for data collection of Lyft L5 dataset. Image from \cite{Lyftl5}}
    \label{fig:seq_data_lyft}
\end{figure}
Most of the popular autonomous driving datasets are of sequential type, but these kind of datasets comes with a drawback of sparse points than other datasets.

Static datasets consists of data collected from a stationary view point by a terrestrial laser scanner.
These kind of datasets capture the static information of the realworld whereas the sequential datasets capture the dynamic movements of the surrounding objects.
Static datasets find their way in applications such as the urban planning, augmented reality and robotics. 
Figure \ref{fig:tls} depcits a terrestrial laser scanner used to capture point cloud of an industrial environment.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.75]{images/TLS.jpg}
    \caption{Terrestrial laser scanner in an industrial environment with the laser scanner mounted on a yellow tripod in the left corner of the floor. Image taken from \cite{tls}}
    \label{fig:tls}
\end{figure}
An advantage with the static datasets, are they can produce highly dense point clouds leading to rich 3D geometric representations.

Last type of 3D LiDAR datasets are synthetic datasets. 
As the name suggests these datasets are generated from the computer simulation. 
Figure \ref{fig:synthetic} depcits a simulated point cloud in a synthtic dataset called SynthCity.
Eventhough synthetic datasets can be generated in large scale with cheap cost, they lack the accuracy in detail when compared to the point clouds generated from real world.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{images/synthcity.png}
    \caption{Illustration of a scene in synthetic dataset called SynthCity. Image taken from \cite{griffiths2019synthcity}}
    \label{fig:synthetic}
\end{figure}

The datasets belonging to the each acquisition type are summed up in  Table \ref{table:3d_lidar_datasets_table}.
Most of the datasets from the Table \ref{table:3d_lidar_datasets_table} are taken from \cite{survey3d} and also as a part of this study, additional new datasets were added to the list.
The newly added datasets include DALES \cite{varney2020dales}, ScanObjectNN \cite{scanobejctnn} in static acquisition mode and AIO Drive \cite{Weng2020_AIODrive}, Toronto3D \cite{tan2020toronto3d} are additions in the sequential mode.
\cite{survey3d} also classifies GTAV (\#cite) dataset as synthetic 3D LiDAR but the corresponding paper doesn't report any LiDAR dataset and proposed only 2D dataset for segmentation.
The limited number of datasets in 3D LiDAR allowed us to study the characteristics of each individual datasets such as each class, data distribution and features of each point in point cloud. 
It is summarized in Table (\#ref) in Appendix (\#chapter number)
%%\section{TODO}
%%\begin{itemize}
%%     \item[$\bullet$] Explain the table
%%    \item[$\bullet$] Discuss why SemanticKITTI and Semantic3D are of our interest     
%%\end{itemize}

\begin{table}[h!]
    %\centering
    \begin{tabular}{c|c|c|c|c|c}%|c}
        \hline
        % acquisition type, dataset, frames, #points, classes, I/O, year
        acquisition mode & dataset & frames & points (in million) & classes & scene type \\  % & pub. year \\
        \hline
        \multirow{7}{*}{static} & Oakland\cite{oakland} & 17 & 1.6 &  44 & outdoor \\ % & 2009 \\ 
                                & Paris-lille-3D\cite{roynard2018paris} & 3 & 143 & 50 & outdoor \\ % & 2018 \\
                                & Paris-rue-Madame\cite{paris_rue_madame} & 2 & 20 & 17 & outdoor \\ %& 2014 \\
                                & S3DIS\cite{Armeni_2016_CVPR_S3DIS} & 5 & 215 & 12 & indoor \\ %& 2016 \\
                                & ScanObjectNN\cite{scanobejctnn} & - & - & 15 & indoor \\ %& 2019 \\
                                & Semantic3D\cite{hackel2017semantic3d} & 30 & 4009 & 8 & outdoor \\ %& 2017 \\
                                & TerraMobilita/IQmulus\cite{TerraMobilita} & 10 & 12 & 15 & outdoor\\ % & 2015 \\
                                & TUM City Campus\cite{gehrung2017approach_tum_campus} & 631 & 41 & 8 & outdoor\\ % & 2016 \\
                                & DALES\cite{varney2020dales} & 40 (tiles) & 492 & 8 & outdoor\\ % & 2021\\
        \hline
        \multirow{7}{*}{sequential} & A2D2\cite{geyer2020a2d2} & 41277 & 1238 & 38 & outdoor\\ % & 2020\\
                                    & AIO Drive\cite{Weng2020_AIODrive} & 100& - & 23 & outdoor\\ % & 2020\\
                                    & KITTI-360\cite{Xie_2016_CVPR_KITTI_360} & 100K & 18000 & 19 & outdoor\\ %& 2020\\
                                    & nuScenes-lidarseg\cite{caesar2020nuscenes} & 40000 & 1400 & 32& outdoor\\ % & 2020\\
                                    & PandaSet\cite{PandaSet} & 16000 & 1844 & 37 & outdoor \\ %& 2020\\
                                    & SemanticKITTI\cite{Behley_2019_ICCV} & 43552 & 4549 & 28 & outdoor \\ %& 2019\\
                                    & SemanticPOSS\cite{pan2020semanticposs} & 2988 & 216 & 14 & outdoor \\ %& 2020\\
                                    & Sydney Urban\cite{de2013unsupervised} & 631 & - & 26 & outdoor\\ % & 2013\\
                                    & Toronto-3D\cite{tan2020toronto3d} & 4 & 78.3& 8& outdoor\\ % & 2020\\

        \hline
        \multirow{1}{*}{synthetic}  & SynthCity\cite{griffiths2019synthcity} & 75000 & 367.9 & 9 & outdoor \\ %& 2019\\
        \hline
    \end{tabular}
    \caption{3D LiDAR datasets classified based on the acquisition type. Table updated from \cite{survey3d}}
    \label{table:3d_lidar_datasets_table}
\end{table}
\newpage
\section{Related work - Models}
\begin{figure}
    \centering
    \includestandalone[width=0.8\linewidth]{images/models_plot}
    \caption{Comparison of 3D semantic segmentation methods performance on SemanticKITTI dataset against the number of parameters. 
             Blue points represent point based methods and red represented projection based methods.}
\end{figure}
\end{document}
