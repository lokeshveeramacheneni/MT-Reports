%!TEX root = ../report.tex

\begin{document}
\chapter{Notes/Remarks}
\section{Related work - Models}

In this section, we will discuss about the methods available for 3D semantic segmentation.
The discussion include a breif peek into traditional 3D semantic segmentation methods and study of deep learning based 3D point cloud segmentation.

Traditional methods involve a complex features extraction and pass these features to a classficiation algorithm such as Support Vector Machines or Random Forests to classify each point the point cloud.
Various authors developed variety of methods to extract the features from the input point cloud.
Some of these methods include segmentation from edge information \cite{bhanu1986range}, construction of complex graph pyramids \cite{koster}.
3D Hough transforms as in \cite{vosselman20013d} and application of RANSAC \cite{schnabel2007efficient} and \cite{tarsha2007hough}.
These traditional methods are now outdated as DNNs proved to better at feature extraction.

\subsection{Deep learning based 3D semantic segmentation}


\begin{table}[h!]
    \begin{tabular}{p{0.25\linewidth}  p{0.5\linewidth} p {0.05\linewidth} p{0.07\linewidth}}
        \hline
        Method & Summary & Type & \#Params \\
        \hline 
        PointNet\cite{Qi_2017_CVPR_pointnet} &  & Point & 3M \\
        \hline
        PointNet++\cite{qi2017pointnet++} & & Point & 6M \\
        \hline
        TangentConv\cite{Tatarchenko_2018_CVPR_tangconv} & & Point & 0.4M\\
        \hline
        SPLATNet\cite{Su_2018_CVPR_splatnet} & & Point & 0.8M \\
        \hline
        Squeezeseg\cite{Sequeseseg_2018} & & Project & 1M \\
        \hline
        SPGraph\cite{SPGraph} & & Point & 0.25M\\
        \hline
        LatticeNet\cite{rosu2019latticenet} & & Point & - \\
        \hline
        SqueezesegV2\cite{SqueezeSegv2} & & Project & 1M \\
        \hline
        RangeNet-21\cite{Milioto2019} & & Project & 25M \\
        \hline
        RangeNet-53\cite{Milioto2019}  & & Project & 50M \\
        \hline
        RangeNet-53++\cite{Milioto2019} & & Project & 50M \\
        \hline
        SqueezesegV3\cite{xu2020squeezesegv3} & & Project & 0.92M \\
        \hline
        RandLA-Net\cite{Hu_2020_CVPR_Randla} & & Point & 0.95M \\
        \hline 
        3DMiniNet\cite{3Dmininet} & & Project & 4M \\
        \hline
        SalsaNet\cite{salsanet2020} & & Project & 6.6M \\
        \hline
        SalsaNext\cite{SalsaNext_2020} & & Project & 6.7M \\
        \hline
        PolarNet\cite{polarnet} & & Project & 14M \\
        \hline
        KPRNet\cite{kochanov2020kprnet} & & Project & 243M \\
        \hline
        SPVNAS\cite{spvnas} & & Point & 2.6M \\
        \hline
        Cylinder3D\cite{zhu2020cylindrical} & & Project & - \\
        \hline
        (AF)2-S3Net\cite{af2s3net} & & Point & - \\
        \hline

    \end{tabular}
\end{table}
\begin{figure}[h!]
    \centering
    \includestandalone[width=0.6\linewidth]{images/models_plot}
    \caption{Comparison of 3D semantic segmentation methods performance on SemanticKITTI dataset against the number of parameters. 
             Blue points represent point based methods and red represented projection based methods.}
\end{figure}


\newpage
\section{Semantic3D}
Semantic3D is a huge 3D benchmark point cloud classification dataset and classified as static dataset.
The dataset consists of nearly 4 billion points which contain variety of scenes in urban and rural setting.
These scenes are taken in places such as markets, dom, stations and fields collected in European streets with terrestrial lasers.
Each point in the point cloud consists of geometric positions (x, y, and z), color (R, G, and B) and intensity values as features.
Example point cloud scenes are provided in \textcolor{red}{\textbf{cite figure}}. 

The dataset consits of 8 classes and they include
\begin{enumerate}
    \item man-made terrain - pavement
    \item natural terrain - grass
    \item high vegetation - large bushes and trees
    \item low vegetation - flowers and bushes less than 2cm in height
    \item buildings - stations, churches, cityhalls
    \item hardscapes - garden walls, banks, fountains
    \item scanning artificats - dynmically moving objects
    \item cars
\end{enumerate}
The distribution of these calsses are given in Figure \ref{fig:sem3ddist}.
From this graph, we can observe that the manmade terrain made most of the dataset because the lidar is placed on street during collection.
As they are near to lidar and it is common with outdoor lidar datasets.
The classes low vegetation, hardscapes, scanning artificats and cars have less number of training points and lower performance from the model on these classes are to be expected.
Also according to \cite{hackel2017semantic3d}, scanning artifacts, cars and hardscapes are toughest classes becuase of variation in obejct shapes.
\cite{survey3d} also proves that the Semantic3D is most diverse dataset in 3D LiDAR data compared to other datasets such as SemanticKITTI and SemanticPOSS.
Becuase of these reasons, we considered using Semantic3D dataset as in distribution training data.
The dataset is available to download on http://www.semantic3d.net/. 
As this is an ongoing benchmark challenge, the labels for the testing data is not available.
We made use of validation set for evaluation purpose which is a subset of trianing set.
\begin{figure}[h!]
    \centering
    \includestandalone[width=0.6\linewidth]{images/Sem3d_points}
    \caption{Distribution of training points in million per class in Semantic3D dataset.}
    \label{fig:sem3ddist}
\end{figure}

\newpage
\section{S3DIS}
S3DIS is an indoor dataset making it an ideal OOD dataset candidatae becuase of the no class overlap with the Semantic3D dataset.
It is only one of two datasets available in indoor LiDAR dataset candidataes. 
The other is ScanObjectNN whose dataset is not available online.
Dataset comprises of scans of three different buildings covering an 6020 square meters.
These scans include areas such as personal offices, restrooms, open spaces, lobbies and hallways.
The scans are generated using Matterport 3D scanner and can be seen in \textcolor{red}{\textbf{cite figure}}.
S3DIS dataset is divided into 12 classes which are further divided into two subclasses.
First subclass include structural elements which consist of \textit{ceiling, floor, window, wall, beam, columns and door}
and latter subclass has common items such as \textit{table, sofa, chair, board and blackboard}.
\end{document}
